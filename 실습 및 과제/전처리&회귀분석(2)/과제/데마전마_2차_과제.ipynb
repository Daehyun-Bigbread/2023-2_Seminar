{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "뉴스 기사 텍스트 데이터를 크롤링하고 분석하는 파이썬 프로그램을 작성하세요.\n",
        "1. 원하는 기사를 찾은 후 해당 페이지 URL에서 뉴스 기사 텍스트 데이터를 크롤링하세요"
      ],
      "metadata": {
        "id": "yamM59qi_yy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUIRgXrY_kT2",
        "outputId": "2c400d9b-a15e-44fe-bbde-4375f2306b5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "pip install beautifulsoup4 requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def crawl_news_article(url):\n",
        "    try:\n",
        "        # 웹페이지에 GET 요청을 보냅니다.\n",
        "        response = requests.get(url)\n",
        "\n",
        "        # 요청이 성공적으로 완료되었는지 확인합니다.\n",
        "        if response.status_code == 200:\n",
        "            # BeautifulSoup을 사용하여 HTML 페이지를 파싱합니다.\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # 원하는 기사 텍스트를 추출합니다.\n",
        "            # 이 부분은 웹 사이트의 구조에 따라 조정해야 할 수 있습니다.\n",
        "            article_text = \"\"\n",
        "            article_elements = soup.find_all('p')  # 예시로 모든 <p> 태그를 추출합니다.\n",
        "            for element in article_elements:\n",
        "                article_text += element.text + \"\\n\"\n",
        "\n",
        "            return article_text\n",
        "        else:\n",
        "            print(f\"HTTP Error {response.status_code}: 페이지를 불러오는 데 문제가 발생했습니다.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"오류 발생: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링할 뉴스 기사의 URL을 입력합니다.\n",
        "    news_article_url = \"https://example.com/news-article-url\"\n",
        "\n",
        "    # 크롤링 함수를 호출하여 뉴스 기사 텍스트를 가져옵니다.\n",
        "    news_article_text = crawl_news_article(news_article_url)\n",
        "\n",
        "    if news_article_text:\n",
        "        # 가져온 기사 텍스트를 출력하거나 분석 작업을 수행할 수 있습니다.\n",
        "        print(news_article_text)\n",
        "    else:\n",
        "        print(\"뉴스 기사 텍스트를 가져오지 못했습니다.\")\n"
      ],
      "metadata": {
        "id": "BDBkFRnF_seM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. 크롤링한 텍스트 데이터를 토큰화하고, 각 단어의 빈도를 계산하세요."
      ],
      "metadata": {
        "id": "2D9BNQbv_10Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "id": "QEvzN7R7AA-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt')  # 필요한 데이터를 다운로드\n",
        "\n",
        "def tokenize_and_count_frequency(text):\n",
        "    # 텍스트 데이터를 토큰화합니다.\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # 각 단어의 빈도를 계산합니다.\n",
        "    word_frequency = Counter(tokens)\n",
        "\n",
        "    return word_frequency\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링한 뉴스 기사 텍스트 데이터를 가져옵니다.\n",
        "    news_article_text = \"여기에 크롤링한 뉴스 기사 텍스트를 넣으세요.\"\n",
        "\n",
        "    # 토큰화 및 빈도 계산 함수를 호출합니다.\n",
        "    word_frequency = tokenize_and_count_frequency(news_article_text)\n",
        "\n",
        "    # 단어 빈도를 출력합니다.\n",
        "    for word, frequency in word_frequency.items():\n",
        "        print(f\"{word}: {frequency}회\")\n"
      ],
      "metadata": {
        "id": "ZPFPNbsPADJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. 가장 빈도가 높은 단어 10개를 찾아 출력하세요."
      ],
      "metadata": {
        "id": "GhQZvaSYAIWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def find_top_words(word_frequency, top_n=10):\n",
        "    # 빈도가 높은 순서대로 정렬된 (단어, 빈도) 튜플의 리스트를 생성합니다.\n",
        "    sorted_word_frequency = sorted(word_frequency.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # 상위 N개의 단어를 선택합니다.\n",
        "    top_words = sorted_word_frequency[:top_n]\n",
        "\n",
        "    return top_words\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 이전 단계에서 계산한 단어 빈도 데이터를 가져옵니다.\n",
        "    word_frequency = {\"단어1\": 100, \"단어2\": 90, \"단어3\": 80, ...}  # 실제 데이터로 대체하세요.\n",
        "\n",
        "    # 가장 빈도가 높은 단어 10개를 찾습니다.\n",
        "    top_words = find_top_words(word_frequency, top_n=10)\n",
        "\n",
        "    # 결과를 출력합니다.\n",
        "    print(\"가장 빈도가 높은 단어 10개:\")\n",
        "    for word, frequency in top_words:\n",
        "        print(f\"{word}: {frequency}회\")\n"
      ],
      "metadata": {
        "id": "n7K4CEoRAFHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "4. 뉴스 기사의 길이를 계산하세요.\n",
        "def calculate_news_article_length(text):\n",
        "    # 뉴스 기사 텍스트의 문자 수를 세어 반환합니다.\n",
        "    return len(text)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링한 뉴스 기사 텍스트 데이터를 가져옵니다.\n",
        "    news_article_text = \"여기에 크롤링한 뉴스 기사 텍스트를 넣으세요.\"\n",
        "\n",
        "    # 뉴스 기사의 길이를 계산합니다.\n",
        "    article_length = calculate_news_article_length(news_article_text)\n",
        "\n",
        "    # 결과를 출력합니다.\n",
        "    print(f\"뉴스 기사의 길이: {article_length} 문자\")\n"
      ],
      "metadata": {
        "id": "M3pb6Y7YAmCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "5. 문장의 평균 길이를 계산하세요.\n",
        "import nltk\n",
        "\n",
        "# NLTK에서 필요한 데이터를 다운로드합니다.\n",
        "nltk.download('punkt')\n",
        "\n",
        "def calculate_average_sentence_length(text):\n",
        "    # 문장 분리기를 사용하여 텍스트를 문장 단위로 분할합니다.\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "    # 각 문장의 길이를 저장할 리스트를 초기화합니다.\n",
        "    sentence_lengths = []\n",
        "\n",
        "    # 각 문장의 길이를 계산하여 리스트에 추가합니다.\n",
        "    for sentence in sentences:\n",
        "        words = nltk.word_tokenize(sentence)\n",
        "        sentence_lengths.append(len(words))\n",
        "\n",
        "    # 문장의 평균 길이를 계산합니다.\n",
        "    if len(sentence_lengths) > 0:\n",
        "        average_length = sum(sentence_lengths) / len(sentence_lengths)\n",
        "    else:\n",
        "        average_length = 0\n",
        "\n",
        "    return average_length\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링한 뉴스 기사 텍스트 데이터를 가져옵니다.\n",
        "    news_article_text = \"여기에 크롤링한 뉴스 기사 텍스트를 넣으세요.\"\n",
        "\n",
        "    # 문장의 평균 길이를 계산합니다.\n",
        "    avg_sentence_length = calculate_average_sentence_length(news_article_text)\n",
        "\n",
        "    # 결과를 출력합니다.\n",
        "    print(f\"문장의 평균 길이: {avg_sentence_length:.2f} 단어\")\n"
      ],
      "metadata": {
        "id": "U0FYfG8IAroK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "6. 기사의 감정을 분석하기 위해 긍정적인 단어와 부정적인 단어의 빈도를 계산하세요.\n",
        "def analyze_sentiment(text):\n",
        "    # 긍정적인 단어와 부정적인 단어 리스트를 정의합니다.\n",
        "    positive_words = [\"good\", \"happy\", \"excellent\", \"positive\", \"wonderful\"]\n",
        "    negative_words = [\"bad\", \"sad\", \"terrible\", \"negative\", \"awful\"]\n",
        "\n",
        "    # 텍스트를 소문자로 변환하여 대소문자 구분을 피합니다.\n",
        "    text = text.lower()\n",
        "\n",
        "    # 텍스트에서 각 단어를 추출합니다.\n",
        "    words = text.split()\n",
        "\n",
        "    # 각 단어의 빈도를 초기화합니다.\n",
        "    positive_word_count = 0\n",
        "    negative_word_count = 0\n",
        "\n",
        "    # 텍스트에서 긍정적인 단어와 부정적인 단어의 빈도를 계산합니다.\n",
        "    for word in words:\n",
        "        if word in positive_words:\n",
        "            positive_word_count += 1\n",
        "        elif word in negative_words:\n",
        "            negative_word_count += 1\n",
        "\n",
        "    return positive_word_count, negative_word_count\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링한 뉴스 기사 텍스트 데이터를 가져옵니다.\n",
        "    news_article_text = \"여기에 크롤링한 뉴스 기사 텍스트를 넣으세요.\"\n",
        "\n",
        "    # 감정 분석을 수행합니다.\n",
        "    positive_count, negative_count = analyze_sentiment(news_article_text)\n",
        "\n",
        "    # 결과를 출력합니다.\n",
        "    print(f\"긍정적인 단어 빈도: {positive_count} 개\")\n",
        "    print(f\"부정적인 단어 빈도: {negative_count} 개\")\n"
      ],
      "metadata": {
        "id": "q8V9ngWNAyaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "7.  뉴스 기사의 주제나 주요 토픽을 분석하기 위해 가장 많이 등장하는 명사를 찾아 출력하세요.\n",
        "pip install nltk"
      ],
      "metadata": {
        "id": "31j6io7YA24A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def extract_nouns(text):\n",
        "    # 텍스트를 소문자로 변환하고 토큰화합니다.\n",
        "    tokens = word_tokenize(text.lower())\n",
        "\n",
        "    # 불용어(stop words)를 제거합니다.\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "    # 명사만 추출합니다.\n",
        "    tagged_tokens = pos_tag(filtered_tokens)\n",
        "    nouns = [word for word, pos in tagged_tokens if pos.startswith('NN')]\n",
        "\n",
        "    return nouns\n",
        "\n",
        "def find_most_common_nouns(text, top_n=10):\n",
        "    nouns = extract_nouns(text)\n",
        "\n",
        "    # 명사의 빈도를 계산합니다.\n",
        "    freq_dist = FreqDist(nouns)\n",
        "\n",
        "    # 가장 많이 등장하는 명사 상위 N개를 가져옵니다.\n",
        "    most_common_nouns = freq_dist.most_common(top_n)\n",
        "\n",
        "    return most_common_nouns\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 크롤링한 뉴스 기사 텍스트 데이터를 가져옵니다.\n",
        "    news_article_text = \"여기에 크롤링한 뉴스 기사 텍스트를 넣으세요.\"\n",
        "\n",
        "    # 가장 많이 등장하는 명사를 찾습니다.\n",
        "    most_common_nouns = find_most_common_nouns(news_article_text, top_n=10)\n",
        "\n",
        "    # 결과를 출력합니다.\n",
        "    print(\"가장 많이 등장하는 명사:\")\n",
        "    for noun, frequency in most_common_nouns:\n",
        "        print(f\"{noun}: {frequency}회\")\n"
      ],
      "metadata": {
        "id": "XQc6mYm8Bc0W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}